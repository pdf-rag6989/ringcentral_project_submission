{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8621bd4-d043-4e78-9ef5-94f6ff3010df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e88c0bff-31dd-4e61-bf31-fbc67b422f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import fitz\n",
    "import re\n",
    "import pandas as pd\n",
    "from img2table.document import PDF\n",
    "from img2table.ocr import TesseractOCR\n",
    "import json\n",
    "import base64\n",
    "import torch\n",
    "from transformers import AutoProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import fitz\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pytesseract\n",
    "from logger import Logger\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from generic import GenericFunction\n",
    "import ssl\n",
    "import certifi\n",
    "os.environ['CURL_CA_BUNDLE'] = './certificates/huggingface.co.pem'\n",
    "ssl._create_default_https_context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/opt/homebrew/bin/tesseract'\n",
    "\n",
    "class PDFProcessor:\n",
    "    \n",
    "    def __init__(self, file_mapping, openai_key,source_pdf_path=\"source_data\", output_folder=\"data_parsed\", images_folder=\"images\", page_range=None):\n",
    "        \"\"\"\n",
    "        Initializes the PDFProcessor class.\n",
    "        \"\"\"\n",
    "        self.generic=GenericFunction\n",
    "        self.logger = Logger()\n",
    "        self.file_mapping = file_mapping\n",
    "        self.source_pdf_path = source_pdf_path\n",
    "        self.output_folder = output_folder\n",
    "        self.page_range = page_range\n",
    "        self.images_folder = images_folder\n",
    "        self.page_range=page_range\n",
    "        self.processor = AutoProcessor.from_pretrained(\"facebook/nougat-base\")\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-base\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        self.api_key = openai_key\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        self.prompt= self.read_prompt(\"./prompts/pdf_parsing.txt\")\n",
    "        self.logger.log_info(\"The prompt is {}\".format(self.prompt))\n",
    "\n",
    "\n",
    "    def read_prompt(self,file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read().strip()\n",
    "            \n",
    "    def get_or_download_pdfs(self):\n",
    "        \"\"\"\n",
    "        Checks if all PDFs and their OCR versions exist locally. Downloads PDFs if not available.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.source_pdf_path, exist_ok=True)\n",
    "        original_files = []\n",
    "        ocr_files = []\n",
    "    \n",
    "        for mapping in self.file_mapping:\n",
    "            for url, file_name in mapping.items():\n",
    "                local_path = os.path.join(self.source_pdf_path, file_name)\n",
    "                ocr_path = os.path.join(self.source_pdf_path, file_name.replace(\".pdf\", \"_OCR.pdf\"))\n",
    "    \n",
    "                if os.path.exists(ocr_path):\n",
    "                    self.logger.log_info(f\"OCR version of the PDF exists locally at: {ocr_path}\")\n",
    "                    ocr_files.append(ocr_path)\n",
    "                else:\n",
    "                    self.logger.log_info(f\"OCR version not found for: {file_name}. Ensure OCR processing is done.\")\n",
    "    \n",
    "                if os.path.exists(local_path):\n",
    "                    self.logger.log_info(f\"Original PDF already exists locally at: {local_path}\")\n",
    "                    original_files.append(local_path)\n",
    "                else:\n",
    "                    self.logger.log_info(f\"Downloading PDF from URL: {url}\")\n",
    "                    response = requests.get(url)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(local_path, \"wb\") as f:\n",
    "                            f.write(response.content)\n",
    "                        self.logger.log_info(f\"Downloaded PDF to: {local_path}\")\n",
    "                        original_files.append(local_path)\n",
    "                    else:\n",
    "                        self.logger.log_error(f\"Failed to download PDF from {url}\")\n",
    "                        raise Exception(f\"Failed to download PDF. HTTP Status Code: {response.status_code}\")\n",
    "    \n",
    "        return original_files, ocr_files\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Cleans the input text by joining lines and handling non-string inputs.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return str(text) if text is not None else \"\" \n",
    "        lines = text.split(\"\\n\")\n",
    "        cleaned_lines = []\n",
    "        buffer = \"\"  \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if re.search(r\"[.!?]$\", line) or line.startswith(\"*\"):\n",
    "                if buffer:\n",
    "                    cleaned_lines.append(buffer.strip())\n",
    "                    buffer = \"\"\n",
    "                cleaned_lines.append(line)\n",
    "            else:\n",
    "                buffer += \" \" + line\n",
    "        if buffer:\n",
    "            cleaned_lines.append(buffer.strip())\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "\n",
    "    def dataframe_json_to_continuous_text(self,json_list):\n",
    "        \"\"\"\n",
    "        Converts a list of JSON objects (representing DataFrame tables) into continuous text.\n",
    "        \"\"\"\n",
    "        continuous_text = []\n",
    "        for idx, table_json in enumerate(json_list, start=1):\n",
    "            if isinstance(table_json, str):\n",
    "                try:\n",
    "                    table_json = json.loads(table_json)\n",
    "                except json.JSONDecodeError:\n",
    "                    raise ValueError(f\"Invalid JSON string at index {idx}: {table_json}\")\n",
    "            if not isinstance(table_json, list) or not all(isinstance(row, dict) for row in table_json):\n",
    "                raise ValueError(f\"Invalid table format at index {idx}: Expected a list of dictionaries.\")\n",
    "            table_label = f\"Table {idx}: \"\n",
    "            row_texts = []   \n",
    "            for row in table_json:\n",
    "                row_text = \", \".join(f\"{key} is {value}\" for key, value in row.items())\n",
    "                row_texts.append(row_text)\n",
    "            combined_table_text = table_label + \" | \".join(row_texts)\n",
    "            continuous_text.append(combined_table_text)\n",
    "        return \"\\n\".join(continuous_text)\n",
    "\n",
    "\n",
    "    def encode_image(self,image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    def analyze_image_with_openai(self,image_path, openai_model=\"gpt-4o-mini\"):\n",
    "        \"\"\"\n",
    "        Analyzes the content of an image using OpenAI GPT model.\n",
    "        \"\"\"\n",
    "        base64_image = self.encode_image(image_path)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=openai_model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": self.prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def extract_images(self,pdf_path, output_folder=\"images\", dpi=300):\n",
    "        \"\"\"\n",
    "        Extracts full-page and sub-images from a PDF and organizes them in a structured folder.\n",
    "        \"\"\"\n",
    "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        pdf_folder = os.path.join(output_folder, pdf_name)\n",
    "        os.makedirs(pdf_folder, exist_ok=True)\n",
    "        doc = fitz.open(pdf_path)\n",
    "        image_details = {}\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            page_folder = os.path.join(pdf_folder, f\"page-{page_num + 1}\")\n",
    "            os.makedirs(page_folder, exist_ok=True)\n",
    "            pix = page.get_pixmap(dpi=dpi)\n",
    "            full_page_image_path = os.path.join(page_folder, \"full_page_image.png\")\n",
    "            pix.save(full_page_image_path)\n",
    "            sub_image_paths = []\n",
    "            for img_index, img in enumerate(page.get_images(full=True), start=1):\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                ext = base_image[\"ext\"]\n",
    "    \n",
    "                # Save the sub-image with a detailed name\n",
    "                sub_image_name = f\"page_{page_num + 1}_sub_image_{img_index}.{ext}\"\n",
    "                sub_image_path = os.path.join(page_folder, sub_image_name)\n",
    "                with open(sub_image_path, \"wb\") as img_file:\n",
    "                    img_file.write(image_bytes)\n",
    "                sub_image_paths.append(os.path.relpath(sub_image_path, start=os.getcwd()))\n",
    "            image_details[f\"page-{page_num + 1}\"] = {\n",
    "                \"full_page_image\": os.path.relpath(full_page_image_path, start=os.getcwd()),\n",
    "                \"sub_images\": sub_image_paths,\n",
    "            }\n",
    "    \n",
    "        return image_details\n",
    "\n",
    "\n",
    "    def extract_text_from_image(self,image_path):\n",
    "        image = Image.open(image_path)\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "        outputs = self.model.generate(pixel_values, max_length=512, early_stopping=True)\n",
    "        generated_text = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        return generated_text.strip()\n",
    "\n",
    "    def extract_text_from_page(self,pdf_path, page_num):\n",
    "        image_path = self.rasterize_page(pdf_path, page_num)\n",
    "        extracted_text = self.extract_text_from_image(image_path)\n",
    "        return extracted_text\n",
    "\n",
    "    def save_tables_to_excel(self,pdf_path, excel_path=\"tables.xlsx\"):\n",
    "        \"\"\"\n",
    "        Extract all tables from the PDF and save them into an Excel file.\n",
    "        \"\"\"\n",
    "        pdf = PDF(src=pdf_path)\n",
    "        ocr = TesseractOCR(lang=\"eng\")\n",
    "        pdf.to_xlsx(excel_path, ocr=ocr)\n",
    "\n",
    "\n",
    "    def extract_tables_from_excel(self,excel_path, page_num):\n",
    "        \"\"\"\n",
    "        Extract and clean all tables for a specific page from the Excel file.\n",
    "        \"\"\"\n",
    "        tables = []\n",
    "        page_name = f\"Page {page_num}\"\n",
    "        with pd.ExcelFile(excel_path) as xls:\n",
    "            for sheet_name in xls.sheet_names:\n",
    "                if sheet_name.split(\" - \")[0].strip() == page_name:\n",
    "                    df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "                    df = df.applymap(self.clean_text)\n",
    "                    tables.append(df.to_json(orient=\"records\"))\n",
    "        return tables\n",
    "    \n",
    "    def extract_page_title_from_header(self,page, y_threshold=100):\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        top_texts = []\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block and block[\"bbox\"][1] < y_threshold:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        top_texts.append(span[\"text\"])\n",
    "        full_title = \" \".join(top_texts).strip()\n",
    "        cleaned_title = re.sub(r\"[^a-zA-Z0-9\\s&]\", \"\", full_title)\n",
    "        words = cleaned_title.split()\n",
    "        if len(words) > 1:\n",
    "            if words[1].lower() in {\"and\", \"&\"} and len(words) > 2:\n",
    "                return f\"{words[0]} {words[1]} {words[2]}\"\n",
    "            return f\"{words[0]} {words[1]}\"\n",
    "        return words[0] if words else f\"Page {page.number + 1}\"\n",
    "\n",
    "\n",
    "    def rasterize_page(self, pdf_path, page_num, dpi=96):\n",
    "        \"\"\"\n",
    "        Converts a page to an image and saves it under the respective PDF folder.\n",
    "        \"\"\"\n",
    "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        pdf_folder = os.path.join(self.images_folder, pdf_name)\n",
    "        os.makedirs(pdf_folder, exist_ok=True)\n",
    "        page_folder = os.path.join(pdf_folder, f\"page-{page_num + 1}\")\n",
    "        os.makedirs(page_folder, exist_ok=True)\n",
    "    \n",
    "        # Generate the full-page image\n",
    "        pdf = fitz.open(pdf_path)\n",
    "        page = pdf.load_page(page_num)\n",
    "        pix = page.get_pixmap(dpi=dpi)\n",
    "        full_page_image_path = os.path.join(page_folder, \"rasterized_full_page_image.png\")\n",
    "        pix.save(full_page_image_path)\n",
    "        return full_page_image_path\n",
    "\n",
    "    def get_text_tesseract(self,filename):\n",
    "        image = Image.open(filename)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return self.clean_text(text)\n",
    "        \n",
    "    \n",
    "    def process_pdf(self,pdf_path,ocr_pdf,excel_path,image_pdf_path=\"images\", ):\n",
    "        \"\"\"\n",
    "        Processes a PDF and extracts metadata, text, tables, and images into a DataFrame.\n",
    "        \"\"\"\n",
    "    \n",
    "        doc = fitz.open(pdf_path)\n",
    "        metadata = doc.metadata\n",
    "        doc_title = metadata.get(\"title\", \"Unknown Title\")\n",
    "        doc_author = metadata.get(\"author\", \"Unknown Author\")\n",
    "    \n",
    "        image_details = self.extract_images(pdf_path, output_folder=image_pdf_path)\n",
    "        self.save_tables_to_excel(pdf_path, excel_path)\n",
    "    \n",
    "        page_data = []\n",
    "        if self.page_range:\n",
    "            page_num,total_pages=self.page_range\n",
    "        else:\n",
    "            page_num=0\n",
    "            total_pages=len(doc)\n",
    "        for page_num in range(total_pages):\n",
    "            self.logger.log_info(\"Ruuning for PDF : {} and page number {}\".format(pdf_path,page_num))\n",
    "            page = doc[page_num]\n",
    "            page_title = self.extract_page_title_from_header(page)  \n",
    "            page_tables = self.extract_tables_from_excel(excel_path, page_num + 1)\n",
    "            extracted_text = self.extract_text_from_page(ocr_pdf, page_num)\n",
    "            page_image_details = image_details.get(f\"page-{page_num + 1}\", {})\n",
    "            full_page_image = page_image_details.get(\"full_page_image\", \"\")\n",
    "            extracted_text_tesseract=self.get_text_tesseract(full_page_image)\n",
    "            sub_images = page_image_details.get(\"sub_images\", [])\n",
    "            image_description=\"\"\n",
    "            if len(sub_images)>0:\n",
    "                self.logger.log_info(\"Getting image description for path {}\".format(full_page_image))\n",
    "                image_description=self.analyze_image_with_openai(full_page_image)\n",
    "                #print(\"The image description is {}\".format(image_description))\n",
    "            all_images = [full_page_image] + sub_images\n",
    "            table_description=self.dataframe_json_to_continuous_text(page_tables)\n",
    "            combined_text = f\"{extracted_text_tesseract}\\n\\n\"\n",
    "            combined_text += f\"Image Details:\\n{image_description}\\n\\n\" if image_description else \"\"\n",
    "            combined_text += f\"Table Details:\\n{table_description}\" if table_description else \"\"\n",
    "            page_data.append({\n",
    "                \"PDF Name\": os.path.basename(pdf_path),\n",
    "                \"Page Number\": page_num + 1,\n",
    "                \"Page Title\": page_title,\n",
    "                \"Combined Text\":combined_text,\n",
    "                \"Page Text\": extracted_text,\n",
    "                \"Page Text(Tesseract)\": extracted_text_tesseract,\n",
    "                \"All Image Paths\": all_images,\n",
    "                \"Full Page Image Path\": full_page_image,\n",
    "                \"Sub Image Paths\": sub_images,\n",
    "                \"Image Description\": image_description,\n",
    "                \"Table JSON\": page_tables,\n",
    "                \"Table Description\": table_description\n",
    "            })\n",
    "    \n",
    "        df = pd.DataFrame(page_data)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    combined_data = []\n",
    "    from generic import GenericFunction\n",
    "    generic = GenericFunction()\n",
    "    file_mapping = generic.get_value(\"file_mappings\")\n",
    "    open_ai_key = generic.get_value(\"api_keys\")['openai']\n",
    "    page_range = (20, 25)  \n",
    "    processor = PDFProcessor(file_mapping,open_ai_key, page_range=page_range)\n",
    "    original_files, ocr_files = processor.get_or_download_pdfs()\n",
    "    for original_pdf, ocr_pdf in zip(original_files, ocr_files):\n",
    "        excel_path = f\"data_parsed/{os.path.basename(original_pdf).replace('.pdf', '_tables.xlsx')}\"\n",
    "        pdf_df = processor.process_pdf(original_pdf, ocr_pdf, excel_path,\"./images\")\n",
    "        combined_data.append(pdf_df)   \n",
    "    final_combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    output_path = \"data_parsed/Combined_Output.csv\"\n",
    "    final_combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Processed all PDFs and saved consolidated output: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4ac75e0-5edb-47f0-b742-9f52d0f5b222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:16:40,845 [INFO] Loaded config.json from: /Users/deepakn/Desktop/PDF-RAG/config.json\n",
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    896,\n",
      "    672\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 10,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "2024-11-24 16:16:53,857 [INFO] The prompt is Analyze the given image from a car manual and provide a detailed description in a continuous text format. The description should include:\n",
      "1. The type of content in the image (e.g., diagram, instruction, warning sign, specification chart).\n",
      "2. A detailed description of the main components, labels, or features visible in the image (e.g., engine parts, dashboard controls, tire specifications).\n",
      "3. Any numerical data or specifications present in the image (e.g., torque values, pressure limits, dimensions, part numbers).\n",
      "4. Any instructional steps, safety guidelines, or warnings included in the image.\n",
      "5. The context or purpose of the image (e.g., troubleshooting, installation, maintenance).\n",
      "6. Key insights or critical information that would be useful for retrieval or understanding.\n",
      "\n",
      "The output should be in a single paragraph in max 200 words, written in clear and concise language, optimized for embedding in a retrieval system. Also, the focus should be on the image and its relative content, not other text elements.\n",
      "2024-11-24 16:16:53,858 [INFO] OCR version of the PDF exists locally at: source_data/Astor Manual_OCR.pdf\n",
      "2024-11-24 16:16:53,858 [INFO] Original PDF already exists locally at: source_data/Astor Manual.pdf\n",
      "2024-11-24 16:16:53,859 [INFO] OCR version of the PDF exists locally at: source_data/APP-TIAGO-FINAL-OMSB_OCR.pdf\n",
      "2024-11-24 16:16:53,859 [INFO] Original PDF already exists locally at: source_data/APP-TIAGO-FINAL-OMSB.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesseract 5.5.0\n",
      " leptonica-1.85.0\n",
      "  libgif 5.2.2 : libjpeg 8d (libjpeg-turbo 3.0.4) : libpng 1.6.44 : libtiff 4.7.0 : zlib 1.2.12 : libwebp 1.4.0 : libopenjp2 2.5.2\n",
      " Found NEON\n",
      " Found libarchive 3.7.7 zlib/1.2.12 liblzma/5.6.3 bz2lib/1.0.8 liblz4/1.10.0 libzstd/1.5.6\n",
      " Found libcurl/8.6.0 SecureTransport (LibreSSL/3.3.6) zlib/1.2.12 nghttp2/1.61.0\n",
      "Ruuning for start 0 and end 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:17:52,571 [INFO] Getting image description for path images/Astor Manual/page-1/full_page_image.png\n",
      "2024-11-24 16:17:59,150 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruuning for start 1 and end 25\n",
      "Ruuning for start 2 and end 25\n",
      "Ruuning for start 3 and end 25\n",
      "Ruuning for start 4 and end 25\n",
      "Ruuning for start 5 and end 25\n",
      "Ruuning for start 6 and end 25\n",
      "Ruuning for start 7 and end 25\n",
      "Ruuning for start 8 and end 25\n",
      "Ruuning for start 9 and end 25\n",
      "Ruuning for start 10 and end 25\n",
      "Ruuning for start 11 and end 25\n",
      "Ruuning for start 12 and end 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:20:38,569 [INFO] Getting image description for path images/Astor Manual/page-13/full_page_image.png\n",
      "2024-11-24 16:20:45,769 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruuning for start 13 and end 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:20:52,436 [INFO] Getting image description for path images/Astor Manual/page-14/full_page_image.png\n",
      "2024-11-24 16:20:57,789 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruuning for start 14 and end 25\n",
      "Ruuning for start 15 and end 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:21:10,158 [INFO] Getting image description for path images/Astor Manual/page-16/full_page_image.png\n",
      "2024-11-24 16:21:15,925 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruuning for start 16 and end 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:21:18,117 [INFO] Getting image description for path images/Astor Manual/page-17/full_page_image.png\n",
      "2024-11-24 16:21:22,349 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruuning for start 17 and end 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:21:35,304 [INFO] Getting image description for path images/Astor Manual/page-18/full_page_image.png\n",
      "2024-11-24 16:21:41,105 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruuning for start 18 and end 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:21:57,205 [INFO] Getting image description for path images/Astor Manual/page-19/full_page_image.png\n",
      "2024-11-24 16:22:03,216 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruuning for start 19 and end 25\n",
      "Ruuning for start 20 and end 25\n",
      "Ruuning for start 21 and end 25\n",
      "Ruuning for start 22 and end 25\n",
      "Ruuning for start 23 and end 25\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 344\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m original_pdf, ocr_pdf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(original_files, ocr_files):\n\u001b[1;32m    343\u001b[0m     excel_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_parsed/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(original_pdf)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_tables.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 344\u001b[0m     pdf_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     combined_data\u001b[38;5;241m.\u001b[39mappend(pdf_df)   \n\u001b[1;32m    346\u001b[0m final_combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(combined_data, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[38], line 297\u001b[0m, in \u001b[0;36mPDFProcessor.process_pdf\u001b[0;34m(self, pdf_path, ocr_pdf, excel_path, image_pdf_path)\u001b[0m\n\u001b[1;32m    295\u001b[0m page \u001b[38;5;241m=\u001b[39m doc[page_num]\n\u001b[1;32m    296\u001b[0m page_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_page_title_from_header(page)  \n\u001b[0;32m--> 297\u001b[0m page_tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_tables_from_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m extracted_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_text_from_page(ocr_pdf, page_num)\n\u001b[1;32m    299\u001b[0m page_image_details \u001b[38;5;241m=\u001b[39m image_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "Cell \u001b[0;32mIn[38], line 228\u001b[0m, in \u001b[0;36mPDFProcessor.extract_tables_from_excel\u001b[0;34m(self, excel_path, page_num)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sheet_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m page_name:\n\u001b[1;32m    227\u001b[0m             df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(xls, sheet_name\u001b[38;5;241m=\u001b[39msheet_name)\n\u001b[0;32m--> 228\u001b[0m             df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplymap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m             tables\u001b[38;5;241m.\u001b[39mappend(df\u001b[38;5;241m.\u001b[39mto_json(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tables\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/frame.py:10522\u001b[0m, in \u001b[0;36mDataFrame.applymap\u001b[0;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[1;32m  10473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  10474\u001b[0m \u001b[38;5;124;03mApply a function to a Dataframe elementwise.\u001b[39;00m\n\u001b[1;32m  10475\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10515\u001b[0m \u001b[38;5;124;03m1  5  5\u001b[39;00m\n\u001b[1;32m  10516\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  10517\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m  10518\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.applymap has been deprecated. Use DataFrame.map instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m  10519\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m  10520\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m  10521\u001b[0m )\n\u001b[0;32m> 10522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/frame.py:10468\u001b[0m, in \u001b[0;36mDataFrame.map\u001b[0;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[1;32m  10465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(x):\n\u001b[1;32m  10466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39m_map_values(func, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m> 10468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/frame.py:10466\u001b[0m, in \u001b[0;36mDataFrame.map.<locals>.infer\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m  10465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(x):\n\u001b[0;32m> 10466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pdf_rag_2/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[38], line 95\u001b[0m, in \u001b[0;36mPDFProcessor.clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(\u001b[38;5;28mself\u001b[39m,text):\n\u001b[0;32m---> 95\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m     cleaned_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     97\u001b[0m     buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d3a63-88ee-42b7-b6a5-a2ff2c86dc26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pdf_rag_2)",
   "language": "python",
   "name": "pdf_rag_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
